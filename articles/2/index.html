<!DOCTYPE html>
<html lang="en">
<head>
    <title>Music recognition AI</title>
    <meta charset="UTF-8">
    <meta name="description" content="music recognition ai">
    <meta name="keywords" content="music, recognition, ai, machine, learning, sine, waves, sampling, dft, fft, fourier, window, audio, signal, algorithm, shazam, c++, java">

    <link rel="stylesheet" href="../../style.css">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, minimum-scale=1.0">
    <script src="../../js/position_hack.js"></script>
    <script src="../../js/page_manager.js"></script>
    <script>
        //2 = numero ultima pagina
        const pm = new PageManager(2);
    </script>
</head>

<body>
    <h1>Music recognition AI</h1>

    Wouldn't it be cool if a program could find the title of that song you are listening to on the radio? That's when the algorithm described here could come in handy.<br>
    From an abstract point of view, the algorithm works like this:
    <ol>
        <li>it analyzes a collection of songs then stores some computed information of said songs on a database</li>
        <li>you make it listen to a short sample of the song you want to find the name of</li>
        <li>if the song is in the database a title is returned</li>
    </ol>
    <br>
    Actually, I made this project quite a while ago (roughly in 2017) as my final high school thesis. The thing is that only recently I was able to recover both the original Java and \(\LaTeX\) thesis source code.

    <div id="page0" class="page">
        <h2>1. Music and physics</h2>

        A sound is a vibration that propagates through the air and can be understood by ears. MP3 players or computers use headphones or built-in speakers to produce vibrations and make &quot;sounds&quot;. Music is just a kind of signal which can be reproduced using various sinusoidal waveforms and like that must be treated.

        <h3>1.1. Signal scrambling via sines sums</h3>
        A sine wave is characterized by two parameters:
        <ul>
            <li><i>Frequency</i> \(\Longrightarrow\) the number of cycles per unit of time, measured in Hertz</li>
            <li><i>Amplitude</i> \(\Longrightarrow\) the height of the cycle</li>
        </ul>
        <figure style="float:right">
            <img src="img/fig1.1.svg" loading="lazy" width="50px" height="45px" style="width:100%;" alt="sine wave">
            <figcaption>Figure 1.1: A 20Hz sine wave</figcaption>
        </figure>

        A generic sine wave is described by the function:
        \[y(t) = A \cdot \sin(2\pi f t + \varphi)\]

        where:

        <ul>
            <li>\(A\) is the amplitude</li>
            <li>\(f\) is the frequency</li>
            <li>\(t\) is the time</li>
            <li>\(\varphi\) is the phase</li>
        </ul>

        In the figure 1.1 the sine wave is described by the function:
        \[y(t)=1\cdot\sin(2\pi 20 t + 0)=\sin(40\pi t)\]

        <figure style="float:left">
            <img src="img/fig1.2.svg" loading="lazy" width="50px" height="45px" style="width:100%;" alt="three sine waves">
            <figcaption>Figure 1.2: Three different sine waves</figcaption>
        </figure>

        But, in the real world, almost any signal is a weighted sum of sines. For example, take three different sine waves, as follows:
        <ul>
            <li>Frequency 10Hz and amplitude 1 \(\Longrightarrow a(t) = \sin(20\pi t)\)</li>
            <li>Frequency 30Hz and amplitude 2 \(\Longrightarrow b(t) = 2\cdot\sin(60\pi t)\)</li>
            <li>Frequency 60Hz and amplitude 3 \(\Longrightarrow c(t) = 3\cdot\sin(120\pi t)\)</li>
        </ul>
        <figure style="float:right">
            <img src="img/fig1.3.svg" loading="lazy" width="50px" height="45px" style="width:100%;" alt="sum sine wave">
            <figcaption>Figure 1.3: Sum of the three sine waves</figcaption>
        </figure>

        These three functions can be plotted separately as shown in the figure 1.2, or they can be summed up in just a single function to represent a more realistic sound (figure 1.3). 

        <h3>1.2. Spectrogram</h3>
        A <b>spectrogram</b> (figure 1.4) is a visual representation of how the frequencies of a signal vary with time. Usually, a spectrogram is made up of three axis (making a 3D graph):
        <ul>
            <li>Time on the horizontal axis (x)</li>
            <li>Frequencies on the vertical axis (y)</li>
            <li>Amplitude of a given frequency at a given time (described by a color)</li>
        </ul>
        <figure>
            <img src="img/fig1.4.svg" loading="lazy" width="1368px" height="864px" style="width:80%;" alt="spectrogram">
            <figcaption>Figure 1.4: Spectrogram of a real song</figcaption>
        </figure>

        <h3>1.3. Digitalization</h3>
        Nowadays the most common way to listen to music is using a digital file (such as an MP3 file or a FLAC file and
        so on). However, a sound is an analog phenomenon that needs to be converted into a digital representation, in order to be easily recorded and stored.

        <h4>1.3.1. Sampling</h4>
        Analog signals are continuous signals, which means that given two boundaries in the signals there is always a point between them. But in the digital world is not so affordable to store an infinite amount of data, so the analog signal needs to be reduced to a <b>discrete-time signal</b>. This process is called <b>sampling</b>. It is quite simple: an instantaneous value of the continuous signal is taken every <b>T</b> seconds. T is called <b>sampling period</b> and it should be short enough so that the digital song sounds like the analog one.<br>
        The <b>sampling rate</b> or <b>sampling frequency</b> \(f_s\) is the number of samples obtained in one second, given by the formula:
        \[f_s=\frac{1}{T}\]

        The standard sampling rate for digital music is usually 44100 Hz. The reason behind that number lies in the Nyquist-Shannon theorem which can be expressed as:
        <blockquote>
            <div class="title">Nyquist-Shannon theorem</div>
            Suppose the highest frequency component for a given analog signal is \(f_{max}\), the sampling rate must be at least \(2 f_{max}\).
        </blockquote>

        There is some theory involved in the theorem, but it states that given an analog signal, it needs at least 2 points per cycle to be correctly identified. So, since the human ears can listen to signals whose frequency is between 20 Hz and 20 kHz, taking the highest boundary and multiplying it by 2, it will give 40 kHz, which is close enough to the standardized 44.1 kHz (figure 1.5).

        <figure>
            <img src="img/fig1.5.svg" loading="lazy" width="109px" height="68px" style="width:70%;" alt="sampling example">
            <figcaption>Figure 1.5: Sampling example of a 30 Hz signal</figcaption>
        </figure>

        <h4>1.3.2. Quantization</h4>
        With the sampling process, the signal is not fully digital: the time resolution became discrete but what about signal amplitude? The amplitude of a signal represents its loudness, and in an analog world it is continuous, so there must be a way to make it discrete. This process is called <b>quantization</b>. The quantization resolution is measured in bits, also known as <b>bit depth</b> of a song.
        <br>
        Taking a depth of 3 bits means that the loudness of the song can vary between 0 and 23 âˆ’1 , so there are just 8 quantization levels to represent the loudness of the whole song (figure 1.6).

        <figure>
            <img src="img/fig1.6.svg" loading="lazy" width="94px" height="48px" style="width:70%;" alt="3 bit quantization">
            <figcaption>Figure 1.6: 3-bit quantization example</figcaption>
        </figure>

        The higher the bit depth, the better the amplitude is approximated. The standard quantization is coded on <b>16 bits</b>. For instance, in the same previous analog signal with a 16-bit quantization, the amplitude can assume \(2^{16}\) values, which will give a much more precise accuracy (figure 1.7).

        <figure>
            <img src="img/fig1.7.svg" loading="lazy" width="94px" height="48px" style="width:70%;" alt="16 bit quantization">
            <figcaption>Figure 1.7: 16-bit quantization example</figcaption>
        </figure>

        <h3>1.4. Computing the spectrum</h3>
        The previous sections should have given enough information to proceed to the real problem: how to break down a complex audio signal into pure sine waves with their own parameters.

        <h4>1.4.1. Discrete Fourier Transform</h4>
        The <b>DFT</b> (Discrete Fourier Transform) applies to discrete signals and gives a discrete spectrum.
        \[X(n)=\sum_{t=0}^{N-1} x(t)e^{-i\frac{2\pi t n}{N}} \tag{1}\]
        Where:
        <ul>
            <li>\(N\) is the size of the <b>window</b>: the number of samples that composed the signal</li>
            <li>\(X(n)\) is the <b>n<sup>th</sup> bin of frequencies</b></li>
            <li>\(x(t)\) is the <b>t<sup>th</sup> sample of the audio signal</b></li>
        </ul>

        The interpretation is that the vector \(x\) represents the signal level at various points in time, the vector \(X\) represents the signal level at various frequencies. What the formula \((1)\) states is that the signal level at frequency \(n\) is equal to the sum of the signal level at each time \(t\) multiplied by a complex exponential (figure 1.8).
        <br>
        <figure>
            <img src="img/fig1.8.svg" loading="lazy" width="115px" height="40px" style="width:70%;" alt="FT representation">
            <figcaption>Figure 1.8: Visual representation of FT</figcaption>
        </figure>
        For example, take an audio signal with 512-sample window, this formula must be applied 512 times:
        <ul>
            <li>Once for \(n = 0\) to compute the 0<sup>th</sup> bin of frequencies</li>
            <li>Once for \(n = 1\) to compute the 1<sup>st</sup> bin of frequencies</li>
            <li>...</li>
            <li>Once for \(n = 511\) to compute the 511<sup>th</sup> bin of frequencies</li>
        </ul>
        A bin of frequencies is a group of frequencies among two boundaries.<br>
        The reason why the DFT can compute bins of frequencies and not exact frequencies is quite simple: the DFT gives a <b>discrete spectrum</b>. A bin of frequencies is the smallest unit of frequencies the DFT can compute and the size of the bin is called <b>spectral resolution</b> or <b>frequency resolution</b> which is given by the formula:
        \[B_S=\frac{F_S}{N}\]

        Where:
        <ul>
            <li>\(B_S\) is the <b>bin size</b></li>
            <li>\(F_S\) is the <b>sampling rate</b></li>
            <li>\(N\) is <b>the size of the window</b></li>
        </ul>

        For instance, taking a sampling rate of 8000 Hz and a window size of 512, the bin size will be of <b>15.6 Hz</b>, so:
        <ul>
            <li>The 0<sup>th</sup> bin contains the frequencies between 0 Hz and 15.6 Hz</li>
            <li>The 1<sup>st</sup> bin contains the frequencies between 15.6 Hz and 31.2 Hz</li>
            <li>And so on</li>
        </ul>
        
        A particularity for a real-valued signal (such as an audio recording) is that <b>only half of the bins computed by the DFT are needed</b> since the output of the DFT is symmetric. In this case, <i>fewer calculations</i> can be made by exploiting this property, which goes under the name of <b>conjugate complex symmetry</b>.

        <blockquote>
            <div class="title">Conjugate complex symmetry of the DFT</div>
            If a function \(x(t)\) is real valued then:
            \[X(N-n)=X^*(n) \tag{2}\]
            where:
            <ul>
                <li>\(X(\odot)\) is the output of the DFT applied to \(x(t)\)</li>
                <li>\((\odot)*\) denotes the conjugate</li>
            </ul>
            <div class="title">Proof</div>
            Insert \((1)\) in the property \((2)\):
            $$\begin{aligned}
                X(n) &= \sum_{t=0}^{N-1} x(t)e^{-i\frac{2\pi t n}{N}} \\
                X(N-n) &= \sum_{t=0}^{N-1} x(t)e^{-i\frac{2\pi t (N-n)}{N}} \\
                    &= \sum_{t=0}^{N-1} x(t)e^{-i 2\pi t}e^{i\frac{2\pi t n}{N}} \\
                    &= \sum_{t=0}^{N-1} x(t)e^{i\frac{2\pi t n}{N}} \\
                    &= \left(\sum_{t=0}^{N-1} x(t)e^{-i\frac{2\pi t n}{N}}\right) \\
                    &= X^*(n)
            \end{aligned}$$
        </blockquote>

        Hence, if the window size is equal to 512:
        <img src="img/sketch_dft.svg" loading="lazy" width="109px" height="12px" style="width:70%;" alt="sketch DFT">

        So, the DFT algorithm needs to be repeated only half times the window size (256 times in this example).<br>
        To be accurate, most real-DFT implementations outputs an \(N/2 + 1\) length array, where \(N\) is the window size. Taking, as always, a sampling rate of 8000 Hz and a window size of 512 (with the bin size being 15.6 Hz):
        <ul>
            <li>The 0<sup>th</sup> bin contains the so-called DC component or offset, being the sum of each sample in the window (see below)</li>
            <li>The 1<sup>st</sup> bin contains the frequencies between 0 Hz and 15.6 Hz</li>
            <li>The 2<sup>nd</sup> bin contains the frequencies between 15.6 Hz and 31.2 Hz</li>
            <li>And so on</li>
        </ul>
        <blockquote>
            <div class="title">Proof: DC component</div>
            Calculate the DFT \((1)\) with \(n=0\):
            $$\begin{aligned}
            X(n) |_{n=0} &= \sum_{t=0}^{N-1} x(t)e^{-i\frac{2\pi t n}{N}}|_{n=0} \\
                &= \sum_{t=0}^{N-1} x(t)
            \end{aligned}$$
        </blockquote>
        The DC component is simply ignored by the algorithm implementation and, in most cases, equals to 0.

        <h4>1.4.2. Window function</h4>
        Now the problem is partially solved: the DFT can be used to obtain the frequencies amplitude for (just to say) the first \(\frac{1}{10}\) second part of the song, for the second, the third and so on.<br>
        The problem is that in this way a rectangular function is implicitly applied: a function that equals 1 for the song portion under analysis and 0 elsewhere (figure 1.9).
        <figure style="float:right">
            <img src="img/fig1.9.svg" loading="lazy" width="49px" height="45px" style="width:100%;" alt="rectangular window">
            <figcaption>Figure 1.9: Rectangular window function example</figcaption>
        </figure>

        <br><br>
        By windowing the audio signal, the audio signal is multiplied by a window function which depends on the piece of the audio signal under analysis. The usage of a window function produces <b>spectral leakage</b>. Spectral leakage is the appearance of new frequencies that does not exist inside the audio signal. The power of the real frequencies is leaked to others frequencies.
        <br><br>
        Spectral leakage cannot be avoided but it can be controlled and reduced by choosing the right window function: there are many different window functions besides the rectangular one. Just to name a few: <i>triangular</i>, <i>Blackman</i>, <i>Hamming</i>, <i>Hann</i> and many others.<br>
        When analyzing unknown very noisy data best choice is the Hann window function, defined by the following formula:
        \[w(n)=\frac{1}{2}\left(1-\cos\left(\frac{2\pi n}{N-1}\right)\right)\]

        Where:
        <ul>
            <li>\(N\) is the size of the window</li>
            <li>\(w(n)\) is the value of the window function at \(n\)</li>
        </ul>

        The aim of this window function is to decrease the amplitude of the discontinuities at the boundaries of a given piece of an audio signal (see figure 1.10).

        <figure>
            <img src="img/fig1.10.svg" loading="lazy" width="108px" height="80px" style="width:70%;" alt="signal and window shape">
            <figcaption>Figure 1.10: Shaping the signal with a window function</figcaption>
        </figure>

        <h4>1.4.3. Downsampling and window size</h4>
        <b>Downsampling</b> is the process of reducing the sampling rate of a signal. Also, the window size can be reduced accordingly, taking the highest possible acceptable value (the value which gives an acceptable frequency resolution). For instance, resample a song from 44.1 kHz to 8 kHz and use a window size of just 512: in this way, there are fewer samples to analyze and the DFT is applied less frequently.<br>
        The only difference is that the resampled song will only have frequencies from 0 to 4 kHz (see the Nyquist-Shannon theorem), but the most important part of the song is still present in this range of frequencies.
    </div>

    <div id="page1" class="page">
        <h2>2. The actual algorithm</h2>

        Now it is the time to put everything together and start coding the algorithm. The aim of the software is to record a small portion of a song and find its title.<br>
        The following sections will cover a global overview of the algorithm, then the actual process involved in the song scoring.<br>
        Take into account that, at this abstraction level, some obvious parts (such as the track file reading routines) will not be described, since they are standard pieces of code.

        <h3>2.1. Global overview</h3>
        An <b>audio fingerprint</b> is a digital summary that can be used to identify an audio sample. In the figure 2.1 a simplified architecture of the scoring algorithm is represented.<br><br>

        <figure>
            <img src="img/fig2.1.svg" loading="lazy" width="91px" height="48px" style="width:60%;" alt="general overview scheme">
            <figcaption>Figure 2.1: General overview scheme</figcaption>
        </figure>

        On the <b>server side</b>:
        <ul>
            <li>The algorithm computes the fingerprints of the input tracks</li>
            <li>The computed fingerprints are stored in the database</li>
        </ul>

        On the <b>client side</b>:
        <ul>
            <li>The current playing music is recorded for a couple of seconds</li>
            <li>The algorithm computes the fingerprints of the recording</li>
            <li>The fingerprints are sent to the server</li>
            <li>The server analyzes the fingerprints and possibly outputs a song title</li>
        </ul>

        In the following sections the algorithm is being described.

        <h3>2.2. Spectrogram creation</h3>
        The first step to analyze the audio is to create the spectrogram. The process is described in the following lines:
        <ul>
            <li>A 512-sized Hann window function is computed</li>
            <li>The audio signal is divided in 512-sized windows with a 50% overlap (figure 2.2)</li>
            <li>Each audio window is multiplied by the Hann precomputed window</li>
            <li>The DFT is computed for each audio window and added into a vector which represents the spectrogram</li>
        </ul>

        <figure>
            <img src="img/fig2.2.svg" loading="lazy" width="110px" height="68px" style="width:70%;" alt="audio windowing with overlap">
            <figcaption>Figure 2.2: Audio windowing with overlap</figcaption>
        </figure>

        <h3>2.3. Peaks finder and fingerprinting</h3>
        The spectrogram is then processed in order to obtain its most significant information. It is divided into a sort of a grid where each cell of the grid has the following size:
        <ul>
            <li>Width equals to C</li>
            <li>Height equals to a range of frequencies (called <b>band</b>)</li>
        </ul>
        Take the simplified figure 2.3 where:
        <ul>
            <li>\(C = 4\)</li>
            <li>The band size is always 100 Hz</li>
        </ul>

        <figure>
            <img src="img/fig2.3.svg" loading="lazy" width="119px" height="69px" style="width:70%;" alt="spectrogram division">
            <figcaption>Figure 2.3: Spectrogram division (each color is a band)</figcaption>
        </figure>

        In the actual algorithm \(C = 32\) and the bands lengths follow a logarithmic scale (see <a href="#DividingSpectrogramInBands">Dividing the spectrogram in bands</a>).
        <br>
        For each cell the algorithm finds and stores the 3 most powerful frequencies in a vector (figure 2.4).

        <figure>
            <img src="img/fig2.4.svg" loading="lazy" width="110px" height="68px" style="width:70%;" alt="three points per cell spectrogram">
            <figcaption>Figure 2.4: Three points per cell spectrogram</figcaption>
        </figure>

        <h4 id="DividingSpectrogramInBands">2.3.1. Dividing the spectrogram in bands</h4>
        There exist at least a few ways to divide the spectrogram in meaningful frequency ranges, but the most interesting one is the mel scale. It was experimentally discovered that the higher the frequency the hardest for a human hear to notice the difference between relatively close but different frequencies.<br>
        In other words, humans perceive frequencies on a <i>logarithmic scale</i>, so that the difference between 500 and 1000 Hz is noticeable, but this is not true for the difference between 10000 and 10000 Hz.<br>
        The formula to convert between the Hertz (linear) scale and the <b>mel</b> (logarithmic) scale is defined as:

        \[f(m)=700\left(10^{\frac{m}{2595}}-1\right) \tag{3}\]

        where:
        <ul>
            <li><b>\(m\)</b> is the mel frequency</li>
            <li><b>\(f\)</b> is the Hertz frequency</li>
        </ul>
        The plotted formula is represented in figure 2.5.

        <figure style="float:right">
            <img src="img/fig2.5.svg" loading="lazy" width="49px" height="44px" style="width:100%;" alt="mel scale to hertz mapping">
            <figcaption>Figure 2.5: Mel scale to Hertz mapping</figcaption>
        </figure>

        The algorithm follows this approach:
        <ol>
            <li>A starting mel value \(\alpha\) is chosen</li>
            <li>A mel band size \(\delta\) is chosen</li>
            <li>
                Compute the mel bands boundaries as:
                \[\left\{0, \alpha, \alpha + \delta, \alpha + 2\delta, ..., \alpha + k\delta \right\}\
            </li>
            <li>
                Convert the mel band boundaries back to Hertz with formula \(3\):
                \[\left\{0, f(\alpha), f(\alpha+\delta), f(\alpha+2\delta), ..., f(\alpha+k\delta)\right\}\]
            </li>
        </ol>
        
        Each band will contain roughly the same &quot;amount of information&quot; from a listener point of view.

        <h3>2.4. Links structure</h3>
        Now, having a way to compute a frequency summary of an audio signal, the recognition process should be straight enough: each point of the recording should be compared with the points of the full song. Though it works well, this simple approach requires a lot of computation time.<br>
        Instead of comparing each point one by one, the idea is to look for multiple points at the same time. This group of points is called links, and it represents a relationship between a point and some others. In order to be sure that both the recording and the full song will generate the same links, a rule must be defined: spectrogram points must be sorted in ascending order according to the window they belong to.
        <br><br>
        So, given a point \(\alpha\) (called address) and an initially empty set of points A, the algorithm will put in A all the other points \(\beta\) whose absolute window difference compared to the point \(\alpha \) is between 1 and 3 and are in the same band as \(\alpha\). In symbols:
        \[ \beta\in A\Leftrightarrow(1\leq\beta\textrm{.window}-\alpha\textrm{.window}&lt;3)\wedge(\beta\textrm{.band}=\alpha\textrm{.band}) \]
        For each point b belonging to A a link is computed and added to a list (figure 2.6). The link structure is the following (figure 2.7):

        \[
            \textrm{link}
            \left\{\begin{matrix}
            \textrm{.hash} = h\left ( \delta_w, \delta_f, \alpha\textrm{.frequency} \right ) \\
            \textrm{.window} = \alpha\textrm{.window}
            \end{matrix}\right.
        \]

        Where:
        <ul>
            <li>\(\delta_w = \beta\textrm{.window} - \alpha\textrm{.window}\)</li>
            <li>\(\delta_f = \beta\textrm{.frequency} - \alpha\textrm{.frequency}\)</li>
            <li>\(h\) is a generic hash function</li>
        </ul>

        The produced links are quite reproducible, even in the presence of noise and codec compression. Another advantage is that in this way all the times are relative.<br>
        On the server side each link is stored in a database along with the song information.

        <figure>
            <img src="img/fig2.6.svg" loading="lazy" width="108px" height="69px" style="width:70%;" alt="links creation">
            <figcaption>Figure 2.6: Links creation</figcaption>
        </figure>
        <figure>
            <img src="img/fig2.7.svg" loading="lazy" width="108px" height="68px" style="width:70%;" alt="link structure">
            <figcaption>Figure 2.7: Link structure</figcaption>
        </figure>

        <h3>2.5. Scoring</h3>
        It is assumed that the full song links are in the database. Now a client, after recording a small piece of song, computes on its own the links of the recording and sends them to the server. The server has to:
        <ol>
            <li>Put the recording links in a temporary in-memory table</li>
            <li>Relate the full songs links table with the recording table if they share the same hash</li>
            <li>Count the tuples grouped by the time difference between the recording links and the full song links (links of the same song share the same time difference) and the song id</li>
            <li>Descending sort the tuples according to the count field</li>
            <li>The topmost tuple obtained contains the id of the most likely matching</li>
        </ol>
    </div>

    <div id="page2" class="page">
        <h2>3. Show me the code</h2>
        There are two implementations of this same algorithm. You can find both on github:
        <ul>
            <li><a href="https://github.com/davide99/fin_cpp">C++17 version</a></li>
            <li><a href="https://github.com/davide99/fin">Java 8 version</a></li>
        </ul>
        <br>
        Please keep in mind that I'm not a C++ expert, so wear your goggles before visiting my repo.
    </div>

    <div id="navigation-bar">
        <a href="#" onclick="pm.prevPage();" id="navigate-left">&lt;&lt; Previous</a>
        <a href="#" onclick="pm.nextPage();" id="navigate-right">Next &gt;&gt;</a>
    </div>

    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
</body>
</html>